Got it. Here’s a **clean, production-minded plan** that hits your constraints (few API calls, no duplicates, 3-day backfill, Azure ML for features, Python 3.13, Codex-assisted dev), plus the hard truth about the **Azure IP ban**.

## 0) Reality check: Azure egress vs NBA blocking

A lot of people report that **stats.nba.com blocks major cloud IP ranges** (AWS, Heroku, and “most cloud-based IPs”), which lines up with your “Azure public IP ranges are blocked” constraint. ([Medium](https://medium.com/%40inman.justin/working-around-nba-coms-ip-ban-for-cloud-hosted-nba-api-apps-90326ab2632c?utm_source=chatgpt.com "Working Around NBA.com's IP Ban for Cloud-Hosted ..."))

**Implication:** a “pure Azure outbound HTTP call direct to stats.nba.com” is likely to fail.

**Workarounds that still keep the project Azure-centered:**

1. **Route requests through a non-Azure egress** (an HTTP proxy you control outside Azure). `nba_api` explicitly supports per-request proxy settings and custom headers. ([PyPI](https://pypi.org/project/nba_api/1.1.10/?utm_source=chatgpt.com "nba_api"))

2. If you want to avoid proxy fragility, use a **paid/official data provider** as a swap-in source later (not your first choice, but it removes the IP war entirely).

I’d build the pipeline assuming (1), with a clean abstraction so you can swap sources later.

---

## 1) The “fewest calls possible” ingestion strategy

Your best friend is **season-level pulls** rather than “call per game, call per team”.

A practical pattern (used by others in the wild) is **TeamGameLogs per season**. It can pull game logs for a season in a single request (or a very small number of requests), and you can concatenate seasons. ([Medium](https://medium.com/%40adrienpeltzer_17089/using-pythons-nba-api-to-look-at-recent-playoff-history-eca772e97890?utm_source=chatgpt.com "Using python's NBA_API to look at recent Playoff history"))

For dimensions (teams, players), prefer the static datasets (`nba_api.stats.static`) to avoid extra HTTP. ([Medium](https://medium.com/%40ryanchristopherhood/nba-data-pipeline-api-to-cloud-hosted-db-to-power-bi-report-4c40c0a47366?utm_source=chatgpt.com "NBA Data Pipeline: API to Cloud Hosted DB to Power BI ..."))

Also note: nba_api itself warns the NBA APIs are “largely undocumented and change frequently,” so keep your ingestion tolerant to schema drift. ([PyPI](https://pypi.org/project/nba_api/1.1.10/?utm_source=chatgpt.com "nba_api"))

---

## 2) Concrete, testable stages (6 stages)

### Stage 1 — Infrastructure baseline (Azure-only, repeatable)

**Deliverable:** ARM template deploys **Azure SQL** (+ firewall rules for dev), plus a place for secrets (Key Vault recommended).  
**Test:** `az deployment group create ...` succeeds; DB reachable from your dev IP.

### Stage 2 — Database schema with anti-duplicate guarantees

**Deliverable:** T-SQL DDL with keys + unique constraints.  
**Test:** inserting the same (game_id, team_id) twice fails.

### Stage 3 — Backfill 6 seasons into Azure SQL

**Deliverable:** a containerized Python backfill job:

- seasons: pick **2019-20 through 2024-25** (6 seasons) and optionally partial 2025-26 later

- writes to SQL via **idempotent upsert**  
  **Test:** row counts are stable across reruns; no duplicates.

### Stage 4 — Daily incremental job with 3-day backfill window

**Deliverable:** scheduled job runs daily and re-pulls “current season” (or last N days if you choose), then upserts.  
**Test:** if you delete a known game row, next run restores it; rerun does not change counts except for newly available games.

### Stage 5 — Azure ML feature engineering + training pipeline

**Deliverable:** Azure ML pipeline that:

- reads base tables from Azure SQL

- generates rolling-window features in Python

- trains two targets:
  
  - `home_win` classification
  
  - `home_margin` regression (home points minus away points)

- logs metrics to Azure ML (MLflow under the hood)  
  **Test:** pipeline run produces registered model + metrics + artifacts.

### Stage 6 — Scheduled evaluation (no live betting needed)

**Deliverable:** a weekly or daily evaluation job that re-scores a holdout season and tracks drift.  
**Test:** metrics trend visible over time.

---

## 3) Database model recommendation (Azure SQL / T-SQL)

Keep this **minimal and clean** for Stage 1, then expand.

### Core tables

**`dbo.dim_team`**

- `team_id` INT PK

- `abbreviation`, `full_name`, `is_active`, etc.

**`dbo.dim_game`**

- `game_id` VARCHAR(12) PK (string to preserve leading zeros, if any)

- `season` VARCHAR(7) like `2023-24`

- `game_date` DATE

- `home_team_id` INT

- `away_team_id` INT

- `home_pts` SMALLINT NULL

- `away_pts` SMALLINT NULL

- `game_status` VARCHAR(20) NULL (Final, etc.)

- Unique constraint on (`game_date`, `home_team_id`, `away_team_id`) as a sanity check

**`dbo.fact_team_game`** (this is the “model inputs” table)

- `game_id` VARCHAR(12) NOT NULL

- `team_id` INT NOT NULL

- `is_home` BIT NOT NULL

- `opponent_team_id` INT NOT NULL

- “base boxscore” columns you’ll use for rolling features (PTS, REB, AST, TOV, FG_PCT, etc.)

- PK (`game_id`, `team_id`) ← prevents duplicates by construction

**Optional (recommended):**

- `dbo.etl_run` for observability (run_id, started_at, status, rows_upserted)

This keeps features out of SQL (as you requested) but stores everything needed to compute them later.

---

## 4) T-SQL DDL starter (copy/paste)

```sql
CREATE TABLE dbo.dim_team (
    team_id INT NOT NULL PRIMARY KEY,
    abbreviation VARCHAR(5) NULL,
    full_name VARCHAR(100) NULL,
    is_active BIT NOT NULL DEFAULT 1
);

CREATE TABLE dbo.dim_game (
    game_id VARCHAR(12) NOT NULL PRIMARY KEY,
    season  VARCHAR(7)  NOT NULL,
    game_date DATE NOT NULL,
    home_team_id INT NOT NULL,
    away_team_id INT NOT NULL,
    home_pts SMALLINT NULL,
    away_pts SMALLINT NULL,
    game_status VARCHAR(20) NULL,
    CONSTRAINT UQ_dim_game UNIQUE (game_date, home_team_id, away_team_id)
);

CREATE TABLE dbo.fact_team_game (
    game_id VARCHAR(12) NOT NULL,
    team_id INT NOT NULL,
    is_home BIT NOT NULL,
    opponent_team_id INT NOT NULL,

    wl CHAR(1) NULL,
    min TINYINT NULL,
    pts SMALLINT NULL,
    reb SMALLINT NULL,
    ast SMALLINT NULL,
    tov SMALLINT NULL,

    fg_pct  DECIMAL(6,5) NULL,
    fg3_pct DECIMAL(6,5) NULL,
    ft_pct  DECIMAL(6,5) NULL,

    plus_minus SMALLINT NULL,

    CONSTRAINT PK_fact_team_game PRIMARY KEY (game_id, team_id),
    CONSTRAINT FK_fact_team_game_game FOREIGN KEY (game_id) REFERENCES dbo.dim_game(game_id),
    CONSTRAINT FK_fact_team_game_team FOREIGN KEY (team_id) REFERENCES dbo.dim_team(team_id)
);
```

---

## 5) Azure compute choice (Python 3.13 + scheduled)

Python 3.13 is workable in Azure land, but pick the service carefully:

- **Azure Functions:** Python 3.13 is GA, but **not supported on the Consumption plan**, so you’d need Premium (or run as a container). ([Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-python?utm_source=chatgpt.com "Python developer reference for Azure Functions"))

- **Azure Container Apps Jobs:** great fit for “run a container daily”, and easy to standardize with Docker.

- **Azure ML Pipelines:** best place for feature engineering + training, and the `azure-ai-ml` SDK is tested with Python 3.13. ([Microsoft Learn](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python&utm_source=chatgpt.com "Azure ML Package client library for Python"))

Given your goals:

- Ingestion: **Container Apps Job** (scheduled)

- Features + modeling: **Azure ML pipeline**

---

## 6) ARM template for Azure SQL (minimal, parameterized)

This is a lean starter you can expand (Key Vault, private endpoints, etc.):

```json
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "location": { "type": "string", "defaultValue": "eastus" },
    "sqlServerName": { "type": "string" },
    "sqlAdminLogin": { "type": "string" },
    "sqlAdminPassword": { "type": "secureString" },
    "databaseName": { "type": "string", "defaultValue": "nba" },
    "skuName": { "type": "string", "defaultValue": "GP_S_Gen5_1" }
  },
  "resources": [
    {
      "type": "Microsoft.Sql/servers",
      "apiVersion": "2022-05-01-preview",
      "name": "[parameters('sqlServerName')]",
      "location": "[parameters('location')]",
      "properties": {
        "administratorLogin": "[parameters('sqlAdminLogin')]",
        "administratorLoginPassword": "[parameters('sqlAdminPassword')]",
        "version": "12.0"
      },
      "resources": [
        {
          "type": "databases",
          "apiVersion": "2022-05-01-preview",
          "name": "[parameters('databaseName')]",
          "location": "[parameters('location')]",
          "sku": { "name": "[parameters('skuName')]" },
          "properties": { "collation": "SQL_Latin1_General_CP1_CI_AS" }
        }
      ]
    }
  ]
}
```

---

## 7) How Azure ML should “generate features” (best fit for your rules)

Since you do **not** want features stored in Azure SQL, the clean pattern is:

- Store **base facts** in SQL (`fact_team_game`, `dim_game`)

- Azure ML pipeline step loads base facts into a pandas DataFrame

- Compute features like:
  
  - rolling means over last N games (home and away separately)
  
  - days of rest
  
  - recent offensive/defensive efficiency proxies
  
  - opponent-adjusted versions

- Train, evaluate, register model

If you later want “feature governance,” you can add Azure ML Feature Store, but you can still keep SQL as the base system of record.

---

## 8) Codex usage (practical)

Use Codex for:

- generating the repo skeleton (src layout, settings, CI)

- writing the ingestion module + tests

- writing SQL migrations + MERGE upserts

- writing Azure ML pipeline YAML / SDK job definitions

- building Dockerfiles and Container Apps Job configs

You remain the architect; Codex does the typing.

---

If you want, next step I can produce a **repo skeleton** (folders, module boundaries, configs) and the **exact ingestion algorithm** (including the “proxy-first” HTTP client wrapper so Azure jobs work despite the ban).
